{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function NpzFile.__del__ at 0x000002D450D8B1F8>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 230, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 221, in close\n",
      "    if self.zip is not None:\n",
      "AttributeError: 'NpzFile' object has no attribute 'zip'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "*Preliminary* pytorch implementation.\n",
    "Networks for voxelmorph model\n",
    "In general, these are fairly specific architectures that were designed for the presented papers.\n",
    "However, the VoxelMorph concepts are not tied to a very particular architecture, and we \n",
    "encourage you to explore architectures that fit your needs. \n",
    "see e.g. more powerful unet function in https://github.com/adalca/neuron/blob/master/neuron/models.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "class unet_core(nn.Module):\n",
    "    \"\"\"\n",
    "    [unet_core] is a class representing the U-Net implementation that takes in\n",
    "    a fixed image and a moving image and outputs a flow-field\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, enc_nf, dec_nf, full_size=True):\n",
    "        \"\"\"\n",
    "        Instiatiate UNet model\n",
    "            :param dim: dimension of the image passed into the net\n",
    "            :param enc_nf: the number of features maps in each layer of encoding stage\n",
    "            :param dec_nf: the number of features maps in each layer of decoding stage\n",
    "            :param full_size: boolean value representing whether full amount of decoding \n",
    "                            layers\n",
    "        \"\"\"\n",
    "        super(unet_core, self).__init__()\n",
    "\n",
    "        self.full_size = full_size\n",
    "        self.vm2 = len(dec_nf) == 7\n",
    "\n",
    "        # Encoder functions\n",
    "        self.enc = nn.ModuleList()\n",
    "        for i in range(len(enc_nf)):\n",
    "            prev_nf = 2 if i == 0 else enc_nf[i-1]\n",
    "            self.enc.append(conv_block(dim, prev_nf, enc_nf[i], 2))\n",
    "\n",
    "        # Decoder functions\n",
    "        self.dec = nn.ModuleList()\n",
    "        self.dec.append(conv_block(dim, enc_nf[-1], dec_nf[0]))  # 1\n",
    "        self.dec.append(conv_block(dim, dec_nf[0] * 2, dec_nf[1]))  # 2\n",
    "        self.dec.append(conv_block(dim, dec_nf[1] * 2, dec_nf[2]))  # 3\n",
    "        self.dec.append(conv_block(dim, dec_nf[2] + enc_nf[0], dec_nf[3]))  # 4\n",
    "        self.dec.append(conv_block(dim, dec_nf[3], dec_nf[4]))  # 5\n",
    "\n",
    "        if self.full_size:\n",
    "            self.dec.append(conv_block(dim, dec_nf[4] + 2, dec_nf[5], 1))\n",
    "\n",
    "        if self.vm2:\n",
    "            self.vm2_conv = conv_block(dim, dec_nf[5], dec_nf[6]) \n",
    " \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass input x through the UNet forward once\n",
    "            :param x: concatenated fixed and moving image\n",
    "        \"\"\"\n",
    "        x_enc = [x]\n",
    "        for l in self.enc:\n",
    "            x_enc.append(l(x_enc[-1]))\n",
    "\n",
    "        # Three conv + upsample + concatenate series\n",
    "        y = x_enc[-1]\n",
    "        \n",
    "        for i in range(3):\n",
    "            y = self.dec[i](y)\n",
    "            y = self.upsample(y)\n",
    "            y = torch.cat([y, x_enc[-(i+2)]], dim=1)\n",
    "            \n",
    "\n",
    "        # Two convs at full_size/2 res\n",
    "        y = self.dec[3](y)\n",
    "        y = self.dec[4](y)\n",
    "        \n",
    "\n",
    "        # Upsample to full res, concatenate and conv\n",
    "        if self.full_size:\n",
    "            y = self.upsample(y)\n",
    "            y = torch.cat([y, x_enc[0]], dim=1)\n",
    "            y = self.dec[5](y)\n",
    "            \n",
    "\n",
    "        # Extra conv for vm2\n",
    "        if self.vm2:\n",
    "            y = self.vm2_conv(y)\n",
    "            \n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class cvpr2018_net(nn.Module):\n",
    "    \"\"\"\n",
    "    [cvpr2018_net] is a class representing the specific implementation for \n",
    "    the 2018 implementation of voxelmorph.\n",
    "    \"\"\"\n",
    "    def __init__(self, vol_size, enc_nf, dec_nf, full_size=True):\n",
    "        \"\"\"\n",
    "        Instiatiate 2018 model\n",
    "            :param vol_size: volume size of the atlas\n",
    "            :param enc_nf: the number of features maps for encoding stages\n",
    "            :param dec_nf: the number of features maps for decoding stages\n",
    "            :param full_size: boolean value full amount of decoding layers\n",
    "        \"\"\"\n",
    "        super(cvpr2018_net, self).__init__()\n",
    "    \n",
    "        \n",
    "\n",
    "        dim = len(vol_size)\n",
    "        print('DIM:')\n",
    "        print(dim)\n",
    "\n",
    "        self.unet_model = unet_core(dim, enc_nf, dec_nf, full_size)\n",
    "\n",
    "        # One conv to get the flow field\n",
    "        conv_fn = getattr(nn, 'Conv%dd' % dim)\n",
    "        self.flow = conv_fn(dec_nf[-1], dim, kernel_size=3, padding=1)      \n",
    "\n",
    "        # Make flow weights + bias small. Not sure this is necessary.\n",
    "        nd = Normal(0, 1e-5)\n",
    "        self.flow.weight = nn.Parameter(nd.sample(self.flow.weight.shape))\n",
    "        self.flow.bias = nn.Parameter(torch.zeros(self.flow.bias.shape))\n",
    "\n",
    "        self.spatial_transform = SpatialTransformer(vol_size)\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Pass input x through forward once\n",
    "            :param src: moving image that we want to shift\n",
    "            :param tgt: fixed image that we want to shift to\n",
    "        \"\"\"\n",
    "        #print(np.array(src).shape)\n",
    "        x = torch.cat([src, tgt], dim=1)\n",
    "        #print(x.size())\n",
    "        x = self.unet_model(x)\n",
    "        #print(x.size())\n",
    "        flow = self.flow(x)\n",
    "        #print(flow.size())\n",
    "        y = self.spatial_transform(src, flow)\n",
    "        #print(y.size())\n",
    "\n",
    "        return y, flow\n",
    "\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    [SpatialTransformer] represesents a spatial transformation block\n",
    "    that uses the output from the UNet to preform an grid_sample\n",
    "    https://pytorch.org/docs/stable/nn.functional.html#grid-sample\n",
    "    \"\"\"\n",
    "    def __init__(self, size, mode='bilinear'):\n",
    "        \"\"\"\n",
    "        Instiatiate the block\n",
    "            :param size: size of input to the spatial transformer block\n",
    "            :param mode: method of interpolation for grid_sampler\n",
    "        \"\"\"\n",
    "        super(SpatialTransformer, self).__init__()\n",
    "\n",
    "        # Create sampling grid\n",
    "        vectors = [ torch.arange(0, s) for s in size ] \n",
    "        grids = torch.meshgrid(vectors) \n",
    "        grid  = torch.stack(grids) # y, x, z\n",
    "        grid  = torch.unsqueeze(grid, 0)  #add batch\n",
    "        grid = grid.type(torch.FloatTensor)\n",
    "        self.register_buffer('grid', grid)\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, src, flow):   \n",
    "        \"\"\"\n",
    "        Push the src and flow through the spatial transform block\n",
    "            :param src: the original moving image\n",
    "            :param flow: the output from the U-Net\n",
    "        \"\"\"\n",
    "        new_locs = self.grid + flow \n",
    "\n",
    "        shape = flow.shape[2:]\n",
    "\n",
    "        # Need to normalize grid values to [-1, 1] for resampler\n",
    "        for i in range(len(shape)):\n",
    "            new_locs[:,i,...] = 2*(new_locs[:,i,...]/(shape[i]-1) - 0.5)\n",
    "\n",
    "        if len(shape) == 2:\n",
    "            new_locs = new_locs.permute(0, 2, 3, 1) \n",
    "            new_locs = new_locs[..., [1,0]]\n",
    "        elif len(shape) == 3:\n",
    "            new_locs = new_locs.permute(0, 2, 3, 4, 1) \n",
    "            new_locs = new_locs[..., [2,1,0]]\n",
    "\n",
    "        return nnf.grid_sample(src, new_locs, mode=self.mode)\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    [conv_block] represents a single convolution block in the Unet which\n",
    "    is a convolution based on the size of the input channel and output\n",
    "    channels and then preforms a Leaky Relu with parameter 0.2.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Instiatiate the conv block\n",
    "            :param dim: number of dimensions of the input\n",
    "            :param in_channels: number of input channels\n",
    "            :param out_channels: number of output channels\n",
    "            :param stride: stride of the convolution\n",
    "        \"\"\"\n",
    "        super(conv_block, self).__init__()\n",
    "\n",
    "        conv_fn = getattr(nn, \"Conv{0}d\".format(dim))\n",
    "\n",
    "        if stride == 1:\n",
    "            ksize = 3\n",
    "        elif stride == 2:\n",
    "            ksize = 4\n",
    "        else:\n",
    "            raise Exception('stride must be 1 or 2')\n",
    "\n",
    "        self.main = conv_fn(in_channels, out_channels, ksize, stride, 1)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the conv_block\n",
    "        \"\"\"\n",
    "        out = self.main(x)\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
